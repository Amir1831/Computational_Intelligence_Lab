{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks: Step by Step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Zero-Padding\n",
    "\n",
    "\n",
    "### Exercise 1 - zero_pad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: zero_pad\n",
    "\n",
    "def zero_pad(X, pad):\n",
    "    \"\"\"\n",
    "    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image, \n",
    "    as illustrated in Figure 1.\n",
    "    \n",
    "    Argument:\n",
    "    X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images\n",
    "    pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n",
    "    \n",
    "    Returns:\n",
    "    X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line)\n",
    "    X_pad = np.pad(X, ((0, 0), (pad, pad), (pad, pad), (0, 0)), 'constant', constant_values=0)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape = (4, 3, 3, 2)\n",
      "x_pad.shape = (4, 7, 7, 2)\n",
      "x[1, 1] = [[ 0.90085595 -0.68372786]\n",
      " [-0.12289023 -0.93576943]\n",
      " [-0.26788808  0.53035547]]\n",
      "x_pad[1, 1] = [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9905b66ef0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(4, 3, 3, 2)\n",
    "x_pad = zero_pad(x, 2)\n",
    "print (\"x.shape =\", x.shape)\n",
    "print (\"x_pad.shape =\", x_pad.shape)\n",
    "print (\"x[1, 1] =\", x[1, 1])\n",
    "print (\"x_pad[1, 1] =\", x_pad[1, 1])\n",
    "\n",
    "fig, axarr = plt.subplots(1, 2)\n",
    "axarr[0].set_title('x')\n",
    "axarr[0].imshow(x[0,:,:,0])\n",
    "axarr[1].set_title('x_pad')\n",
    "axarr[1].imshow(x_pad[0,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **x.shape**:\n",
    "        </td>\n",
    "        <td>\n",
    "           (4, 3, 3, 2)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **x_pad.shape**:\n",
    "        </td>\n",
    "        <td>\n",
    "           (4, 7, 7, 2)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **x[1,1]**:\n",
    "        </td>\n",
    "        <td>\n",
    "           [[ 0.90085595 -0.68372786]\n",
    " [-0.12289023 -0.93576943]\n",
    " [-0.26788808  0.53035547]]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **x_pad[1,1]**:\n",
    "        </td>\n",
    "        <td>\n",
    "           [[ 0.  0.]\n",
    " [ 0.  0.]\n",
    " [ 0.  0.]\n",
    " [ 0.  0.]\n",
    " [ 0.  0.]\n",
    " [ 0.  0.]\n",
    " [ 0.  0.]]\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Single step of convolution \n",
    "\n",
    "### Exercise 2 - conv_single_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: conv_single_step\n",
    "\n",
    "def conv_single_step(a_slice_prev, W, b):\n",
    "    \"\"\"\n",
    "    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation \n",
    "    of the previous layer.\n",
    "    \n",
    "    Arguments:\n",
    "    a_slice_prev -- slice of input data of shape (f, f, n_C_prev)\n",
    "    W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)\n",
    "    b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)\n",
    "    \n",
    "    Returns:\n",
    "    Z -- a scalar value, result of convolving the sliding window (W, b) on a slice x of the input data\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    # Element-wise product between a_slice and W. Add bias.\n",
    "    s = a_slice_prev * W + b\n",
    "    # Sum over all entries of the volume s\n",
    "    Z = np.sum(s)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = -23.16021220252078\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "a_slice_prev = np.random.randn(4, 4, 3)\n",
    "W = np.random.randn(4, 4, 3)\n",
    "b = np.random.randn(1, 1, 1)\n",
    "\n",
    "Z = conv_single_step(a_slice_prev, W, b)\n",
    "print(\"Z =\", Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **Z**\n",
    "        </td>\n",
    "        <td>\n",
    "            -23.1602122025\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### 3.3 - Convolutional Neural Networks - Forward pass\n",
    "\n",
    "### Exercise 3 -  conv_forward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: conv_forward\n",
    "\n",
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
    "    hparameters -- python dictionary containing \"stride\" and \"pad\"\n",
    "        \n",
    "    Returns:\n",
    "    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward() function\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve dimensions from A_prev's shape (≈1 line)  \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve dimensions from W's shape (≈1 line)\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "\n",
    "    # Retrieve information from \"hparameters\" (≈2 lines)\n",
    "    stride = hparameters['stride']\n",
    "    pad = hparameters['pad']\n",
    "    \n",
    "    # Compute the dimensions of the CONV output volume using the formula given above. Hint: use int() to floor. (≈2 lines)\n",
    "    n_H = int((n_H_prev - f + 2 * pad) / stride) + 1\n",
    "    n_W = int((n_W_prev - f + 2 * pad) / stride) + 1\n",
    "    \n",
    "    # Initialize the output volume Z with zeros. (≈1 line)\n",
    "    Z = np.zeros((m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Create A_prev_pad by padding A_prev\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    \n",
    "    for i in range(m):                                 # loop over the batch of training examples\n",
    "        a_prev_pad = A_prev_pad[i]                            # Select ith training example's padded activation\n",
    "        for h in range(n_H):                           # loop over vertical axis of the output volume\n",
    "            # Find the vertical start and end of the current \"slice\" (≈2 lines)\n",
    "            vert_start = h * stride\n",
    "            vert_end = vert_start + f\n",
    "            \n",
    "            for w in range(n_W):                       # loop over horizontal axis of the output volume\n",
    "                # Find the horizontal start and end of the current \"slice\" (≈2 lines)\n",
    "                horiz_start = w * stride\n",
    "                horiz_end = horiz_start + f\n",
    "                \n",
    "                for c in range(n_C):                   # loop over channels (= #filters) of the output volume\n",
    "                    # Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell). (≈1 line)\n",
    "                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "                    \n",
    "                    # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. (≈3 line)\n",
    "                    weights = W[: , : ,: , c]\n",
    "                    biases = b[: , : , : , c]\n",
    "                    Z[i, h, w, c] = conv_single_step(a_slice_prev , weights , biases)                         \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Making sure your output shape is correct\n",
    "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Save information in \"cache\" for the backprop\n",
    "    cache = (A_prev, W, b, hparameters)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z's mean = 0.15585932488906465\n",
      "cache_conv[0][1][2][3] = [-0.20075807  0.18656139  0.41005165]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(10, 4, 4, 3)\n",
    "W = np.random.randn(2, 2, 3, 8)\n",
    "b = np.random.randn(1, 1, 1, 8)\n",
    "hparameters = {\"pad\" : 2,\n",
    "               \"stride\": 1}\n",
    "\n",
    "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
    "print(\"Z's mean =\", np.mean(Z))\n",
    "print(\"cache_conv[0][1][2][3] =\", cache_conv[0][1][2][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **Z's mean**\n",
    "        </td>\n",
    "        <td>\n",
    "            0.155859324889\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **cache_conv[0][1][2][3]**\n",
    "        </td>\n",
    "        <td>\n",
    "            [-0.20075807  0.18656139  0.41005165]\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Pooling layer \n",
    "\n",
    "\n",
    "### 4.1 - Forward Pooling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: pool_forward\n",
    "\n",
    "def pool_forward(A_prev, hparameters, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the forward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    hparameters -- python dictionary containing \"f\" and \"stride\"\n",
    "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters \n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve dimensions from the input shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\"\n",
    "    f = hparameters[\"f\"]\n",
    "    stride = hparameters[\"stride\"]\n",
    "    \n",
    "    # Define the dimensions of the output\n",
    "    n_H = int(1 + (n_H_prev - f) / stride)\n",
    "    n_W = int(1 + (n_W_prev - f) / stride)\n",
    "    n_C = n_C_prev\n",
    "    \n",
    "    # Initialize output matrix A\n",
    "    A = np.zeros((m, n_H, n_W, n_C))              \n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    for i in range(int(m)):                           # loop over the training examples\n",
    "        for h in range(n_H):                     # loop on the vertical axis of the output volume\n",
    "            # Find the vertical start and end of the current \"slice\" (≈2 lines)\n",
    "            vert_start = h * stride\n",
    "            vert_end = vert_start + f\n",
    "            \n",
    "            for w in range(n_W):                 # loop on the horizontal axis of the output volume\n",
    "                # Find the vertical start and end of the current \"slice\" (≈2 lines)\n",
    "                horiz_start = w * stride\n",
    "                horiz_end = horiz_start + f\n",
    "                \n",
    "                for c in range (n_C):            # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Use the corners to define the current slice on the ith training example of A_prev, channel c. (≈1 line)\n",
    "                    a_prev_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                    \n",
    "                    # Compute the pooling operation on the slice. \n",
    "                    # Use an if statment to differentiate the modes. \n",
    "                    # Use np.max/np.mean.\n",
    "                    if mode == \"max\":\n",
    "                        A[i, h, w, c] = np.max(a_prev_slice)\n",
    "                    elif mode == \"average\":\n",
    "                        A[i, h, w, c] = np.mean(a_prev_slice)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Store the input and hparameters in \"cache\" for pool_backward()\n",
    "    cache = (A_prev, hparameters)\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(A.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    return A, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "A.shape = (2, 3, 3, 3)\n",
      "A[1, 1] =\n",
      " [[1.96710175 0.84616065 1.27375593]\n",
      " [1.96710175 0.84616065 1.23616403]\n",
      " [1.62765075 1.12141771 1.2245077 ]]\n",
      "\n",
      "mode = average\n",
      "A.shape = (2, 3, 3, 3)\n",
      "A[1, 1] =\n",
      " [[ 0.44497696 -0.00261695 -0.31040307]\n",
      " [ 0.50811474 -0.23493734 -0.23961183]\n",
      " [ 0.11872677  0.17255229 -0.22112197]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(2, 5, 5, 3)\n",
    "hparameters = {\"stride\" : 1, \"f\": 3}\n",
    "\n",
    "A, cache = pool_forward(A_prev, hparameters, mode = \"max\")\n",
    "print(\"mode = max\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A[1, 1] =\\n\", A[1, 1])\n",
    "print()\n",
    "A, cache = pool_forward(A_prev, hparameters, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A[1, 1] =\\n\", A[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "A.shape = (2, 2, 2, 3)\n",
      "A[0] =\n",
      " [[[1.74481176 0.90159072 1.65980218]\n",
      "  [1.74481176 1.6924546  1.65980218]]\n",
      "\n",
      " [[1.13162939 1.51981682 2.18557541]\n",
      "  [1.13162939 1.6924546  2.18557541]]]\n",
      "\n",
      "mode = average\n",
      "A.shape = (2, 2, 2, 3)\n",
      "A[1] =\n",
      " [[[-0.17313416  0.32377198 -0.34317572]\n",
      "  [ 0.02030094  0.14141479 -0.01231585]]\n",
      "\n",
      " [[ 0.42944926  0.08446996 -0.27290905]\n",
      "  [ 0.15077452  0.28911175  0.00123239]]]\n"
     ]
    }
   ],
   "source": [
    "# Case 2: stride of 2\n",
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(2, 5, 5, 3)\n",
    "hparameters = {\"stride\" : 2, \"f\": 3}\n",
    "\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "print(\"mode = max\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A[0] =\\n\", A[0])\n",
    "print()\n",
    "\n",
    "A, cache = pool_forward(A_prev, hparameters, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A[1] =\\n\", A[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Backpropagation in convolutional neural networks\n",
    "\n",
    "\n",
    "### 5.1 - Convolutional layer backward pass \n",
    "\n",
    "#### 5.1.1 - Computing dA:\n",
    "\n",
    "\n",
    "#### 5.1.2 - Computing dW:\n",
    "\n",
    "\n",
    "#### 5.1.3 - Computing db:\n",
    "\n",
    "\n",
    "### Exercise 5 - conv_backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward(), output of conv_forward()\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
    "               numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
    "          numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
    "          numpy array of shape (1, 1, 1, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve information from \"cache\"\n",
    "    (A_prev, W, b, hparameters) = cache\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve dimensions from W's shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    # Retrieve information from \"hparameters\"\n",
    "    stride = hparameters.get(\"stride\")\n",
    "    pad = hparameters.get(\"pad\")\n",
    "    \n",
    "    # Retrieve dimensions from dZ's shape\n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "    \n",
    "    # Initialize dA_prev, dW, db with the correct shapes\n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))                           \n",
    "    dW = np.zeros((f, f, n_C_prev, n_C))\n",
    "    db = np.zeros((1, 1, 1, n_C))\n",
    "\n",
    "    # Pad A_prev and dA_prev\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    dA_prev_pad = zero_pad(dA_prev, pad)\n",
    "    \n",
    "    for i in range(m):                       # loop over the training examples\n",
    "        \n",
    "        # select ith training example from A_prev_pad and dA_prev_pad\n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        da_prev_pad = dA_prev_pad[i]\n",
    "        \n",
    "        for h in range(n_H):                   # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):               # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):           # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\"\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Use the corners to define the slice from a_prev_pad\n",
    "                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "\n",
    "                    # Update gradients for the window and the filter's parameters using the code formulas given above\n",
    "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
    "                    \n",
    "        # Set the ith training example's dA_prev to the unpaded da_prev_pad (Hint: use X[pad:-pad, pad:-pad, :])\n",
    "        dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_mean = 9.608990675868995\n",
      "dW_mean = 10.581741275547563\n",
      "db_mean = 76.37106919563735\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "dA, dW, db = conv_backward(Z, cache_conv)\n",
    "print(\"dA_mean =\", np.mean(dA))\n",
    "print(\"dW_mean =\", np.mean(dW))\n",
    "print(\"db_mean =\", np.mean(db))\n",
    "# print(dA.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Expected Output: **\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **dA_mean**\n",
    "        </td>\n",
    "        <td>\n",
    "            9.60899067587\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **dW_mean**\n",
    "        </td>\n",
    "        <td>\n",
    "            10.5817412755\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **db_mean**\n",
    "        </td>\n",
    "        <td>\n",
    "            76.3710691956\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Pooling layer - backward pass\n",
    "\n",
    "### 5.2.1 Max pooling - backward pass  \n",
    "\n",
    "\n",
    "### Exercise 6 - create_mask_from_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_from_window(x):\n",
    "    \"\"\"\n",
    "    Creates a mask from an input matrix x, to identify the max entry of x.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- Array of shape (f, f)\n",
    "    \n",
    "    Returns:\n",
    "    mask -- Array of the same shape as window, contains a True at the position corresponding to the max entry of x.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈1 line)\n",
    "    mask = x == np.max(x)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  [[ 1.62434536 -0.61175641 -0.52817175]\n",
      " [-1.07296862  0.86540763 -2.3015387 ]]\n",
      "mask =  [[ True False False]\n",
      " [False False False]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(2,3)\n",
    "mask = create_mask_from_window(x)\n",
    "print('x = ', x)\n",
    "print(\"mask = \", mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "**Expected Output:** \n",
    "\n",
    "<table> \n",
    "<tr> \n",
    "<td>\n",
    "\n",
    "**x =**\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "\n",
    "[[ 1.62434536 -0.61175641 -0.52817175] <br>\n",
    " [-1.07296862  0.86540763 -2.3015387 ]]\n",
    "\n",
    "  </td>\n",
    "</tr>\n",
    "\n",
    "<tr> \n",
    "<td>\n",
    "**mask =**\n",
    "</td>\n",
    "<td>\n",
    "[[ True False False] <br>\n",
    " [False False False]]\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 - Average pooling - backward pass \n",
    "\n",
    "### Exercise 7 - distribute_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_value(dz, shape):\n",
    "    \"\"\"\n",
    "    Distributes the input value in the matrix of dimension shape\n",
    "    \n",
    "    Arguments:\n",
    "    dz -- input scalar\n",
    "    shape -- the shape (n_H, n_W) of the output matrix for which we want to distribute the value of dz\n",
    "    \n",
    "    Returns:\n",
    "    a -- Array of size (n_H, n_W) for which we distributed the value of dz\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve dimensions from shape (≈1 line)\n",
    "    (n_H, n_W) = shape\n",
    "    \n",
    "    # Compute the value to distribute on the matrix (≈1 line)\n",
    "    average = np.prod(shape)\n",
    "    \n",
    "    # Create a matrix where every entry is the \"average\" value (≈1 line)\n",
    "    a = (dz/average)*np.ones(shape)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distributed value = [[0.5 0.5]\n",
      " [0.5 0.5]]\n"
     ]
    }
   ],
   "source": [
    "a = distribute_value(2, (2,2))\n",
    "print('distributed value =', a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table> \n",
    "<tr> \n",
    "<td>\n",
    "distributed_value =\n",
    "</td>\n",
    "<td>\n",
    "[[ 0.5  0.5]\n",
    "<br\\> \n",
    "[ 0.5  0.5]]\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3 Putting it together: Pooling backward \n",
    "\n",
    "\n",
    "### Exercise 8 - pool_backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pool_backward(dA, cache, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the backward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- gradient of cost with respect to the output of the pooling layer, same shape as A\n",
    "    cache -- cache output from the forward pass of the pooling layer, contains the layer's input and hparameters \n",
    "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Retrieve information from cache (≈1 line)\n",
    "    (A_prev, hparameters) = cache\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\" (≈2 lines)\n",
    "    stride = hparameters.get(\"stride\")\n",
    "    f = hparameters.get(\"f\")\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape and dA's shape (≈2 lines)\n",
    "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    m, n_H, n_W, n_C = dA.shape\n",
    "    \n",
    "    # Initialize dA_prev with zeros (≈1 line)\n",
    "    dA_prev = np.zeros(A_prev.shape)\n",
    "    \n",
    "    for i in range(m):                       # loop over the training examples\n",
    "        # select training example from A_prev (≈1 line)\n",
    "        a_prev = A_prev[i,:,:,:]\n",
    "        \n",
    "        for h in range(n_H):                   # loop on the vertical axis\n",
    "            for w in range(n_W):               # loop on the horizontal axis\n",
    "                for c in range(n_C):           # loop over the channels (depth)\n",
    "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Compute the backward propagation in both modes.\n",
    "                    if mode == \"max\":\n",
    "                        # Use the corners and \"c\" to define the current slice from a_prev (≈1 line)\n",
    "                        a_prev_slice = a_prev[ vert_start:vert_end, horiz_start:horiz_end, c ]\n",
    "                        \n",
    "                        # Create the mask from a_prev_slice (≈1 line)\n",
    "                        mask = create_mask_from_window(a_prev_slice)\n",
    "                        \n",
    "                        # Set dA_prev to be dA_prev + (the mask multiplied by the correct entry of dA) (≈1 line)\n",
    "                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += mask * dA[i, h, w, c]\n",
    "                        \n",
    "                    elif mode == \"average\":\n",
    "                        # Get the value a from dA (≈1 line)\n",
    "                        da = dA[i, h, w, c]\n",
    "                        \n",
    "                        # Define the shape of the filter as fxf (≈1 line)\n",
    "                        shape = (f ,f )\n",
    "                        \n",
    "                        # Distribute it to get the correct slice of dA_prev. i.e. Add the distributed value of da. (≈1 line)\n",
    "                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += distribute_value(da, shape)\n",
    "                        \n",
    "    ### END CODE ###\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev[1,1] =  [[ 0.          0.        ]\n",
      " [ 5.05844394 -1.68282702]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "mode = average\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev[1,1] =  [[ 0.08485462  0.2787552 ]\n",
      " [ 1.26461098 -0.25749373]\n",
      " [ 1.17975636 -0.53624893]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(5, 5, 3, 2)\n",
    "hparameters = {\"stride\" : 1, \"f\": 2}\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "dA = np.random.randn(5, 4, 2, 2)\n",
    "\n",
    "dA_prev = pool_backward(dA, cache, mode = \"max\")\n",
    "print(\"mode = max\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1])  \n",
    "print()\n",
    "dA_prev = pool_backward(dA, cache, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "mode = max:\n",
    "<table> \n",
    "<tr> \n",
    "<td>\n",
    "\n",
    "**mean of dA =**\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "\n",
    "0.145713902729\n",
    "\n",
    "  </td>\n",
    "</tr>\n",
    "\n",
    "<tr> \n",
    "<td>\n",
    "**dA_prev[1,1] =** \n",
    "</td>\n",
    "<td>\n",
    "[[ 0.          0.        ] <br>\n",
    " [ 5.05844394 -1.68282702] <br>\n",
    " [ 0.          0.        ]]\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "mode = average\n",
    "<table> \n",
    "<tr> \n",
    "<td>\n",
    "\n",
    "**mean of dA =**\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "\n",
    "0.145713902729\n",
    "\n",
    "  </td>\n",
    "</tr>\n",
    "\n",
    "<tr> \n",
    "<td>\n",
    "**dA_prev[1,1] =** \n",
    "</td>\n",
    "<td>\n",
    "[[ 0.08485462  0.2787552 ] <br>\n",
    " [ 1.26461098 -0.25749373] <br>\n",
    " [ 1.17975636 -0.53624893]]\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "convolutional-neural-networks",
   "graded_item_id": "qO8ng",
   "launcher_item_id": "7XDi8"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
